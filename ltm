----------------------------------------------------------------
import numpy as np
import sklearn.datasets
import tensorflow as tf

from sklearn.metrics import accuracy_score 
from tensorflow.keras.losses import mean_squared_error

(x_train,y_train),(x_test,y_test)=tf.keras.datasets.mnist.load_data()

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

x_train=x_train.reshape(60000,784)
x_test=x_test.reshape(10000,784)
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)

import pandas as pd

y_train1 = pd.DataFrame(y_train).astype('str')

y_train1 = pd.get_dummies(y_train1)

y_train1

y_train1 = y_train1.to_numpy(int)

y_train1

def ReLU(x):
    return np.maximum(0,x)

def ReLU_derivative(x):
    return np.where(x>0,1,0)

def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)
    
# def gradientclip(x):
    # return np.where(np.where(x<-10,1,x)>10,1,x)
    
class NeuralNetwork:
    def __init__(self,input_size,hidden1_Size,hidden2_size,hidden3_size,output_size):
        #(hidden_layer1)
        self.w1= np.random.random((input_size,hidden1_Size)) *0.1 - 0.05
        self.b1= np.zeros((hidden1_Size)) 

        #(hidden layer2)      
        self.w2=np.random.random((hidden1_Size,hidden2_size)) *0.1 - 0.05
        self.b2=np.zeros((hidden2_size))

        #(hidden layer3)
        self.w3 =np.random.random((hidden2_size,hidden3_size)) *0.1 - 0.05
        self.b3 =np.zeros((hidden3_size))

        #(output_layer)
        self.w4 =np.random.random((hidden3_size,output_size)) *0.1 - 0.05
        self.b4 =np.zeros((output_size))

    
    def forward(self,x):
        self.x1=np.dot(x,self.w1) + self.b1
        self.y1=ReLU(self.x1) #output of hidden_layer1
        self.x2=np.dot(self.y1,self.w2) + self.b2
        self.y2=ReLU(self.x2)
        self.x3=np.dot(self.y2,self.w3) + self.b3
        self.y3=ReLU(self.x3)
        self.x4=np.dot(self.y3,self.w4) + self.b4
        self.o=tf.nn.softmax(self.x4)
        #print(self.o.numpy())

        return self.o.numpy()

    def backward(self,o,x,y,alpha):
        output_delta = (y-o) * 0.01
        #print(output_delta)
        hidden3_error = output_delta.dot(self.w4.T)
        hidden3_delta=hidden3_error*(ReLU_derivative(self.y3))
        #print(hidden3_delta)
        hidden2_error = hidden3_delta.dot(self.w3.T)
        hidden2_delta= hidden2_error*(ReLU_derivative(self.y2))
        hidden1_error = hidden2_delta.dot(self.w2.T)
        hidden1_delta = hidden1_error*(ReLU_derivative(self.y1))

        self.w4+=(self.y3.T.dot(output_delta)*alpha)
        self.w3+=(self.y2.T.dot(hidden3_delta)*alpha)
        self.w2+=(self.y1.T.dot(hidden2_delta)*alpha)
        self.w1+=(x.reshape(1,784).T.dot(hidden1_delta)*alpha)

        self.b4+=(np.sum(output_delta,0)*alpha)/y.shape[0]
        self.b3+=(np.sum(hidden3_delta,0)*alpha)/y.shape[0]
        self.b2+=(np.sum(hidden2_delta,0)*alpha)/y.shape[0]
        self.b1+=(np.sum(hidden1_delta,0)*alpha)/y.shape[0]

        #print('bias:',self.y1.T.dot(hidden2_delta)*alpha)

    def train(self,x,y_t,y,alpha,epoch):
        LOSS=np.zeros(epoch,'float64')
        ACCURACY=np.zeros(epoch,'float64')
        for j in range(epoch):
            for i in range(len(x)):
                o = self.forward(x[i].reshape(1,784))
                self.backward(o,x[i],y_t[i].reshape(1,10),alpha)
            
            ot=self.forward(x)
            LOSS[j]=np.sum(mean_squared_error(y_t,ot))
            ACCURACY[j]=accuracy_score(y,np.argmax(ot,axis=1))
            print('epoch:',j,'loss:',LOSS[j],'accuracy:',ACCURACY[j])
    
    def predict(self,x):
        o = self.forward(x)
        return np.argmax(o,axis=1)

nn=NeuralNetwork(784,32,25,16,10)

nn.train(x_train,y_train1,y_train,0.05,10)

nn.train(x_train,y_train1,y_train,0.1,20)

accuracy_score(y_test,nn.predict(x_test))

nn.predict(x_test)

--------------------------------------------------------------------------------

import numpy as np

def sigmoid(x):
    return 1/(1 + np.exp(-x))

class LogReg:
    def __init__(self, dimensions):
        self.weights = np.random.random(dimensions + 1) * 0.0001
        
    def forward(self, X):
        X = np.concatenate((X, np.ones((X.shape[0],1))), axis = 1)
        return sigmoid(np.dot(X, self.weights))
    
    def backpropagate(self, X, Y):
        pred = self.forward(X)
        X = np.concatenate((X, np.ones((X.shape[0],1))), axis = 1)
        error = pred - Y
        dw = 0.001 * np.dot(X.T, error)
        self.weights -= dw
        
 lr = LogReg(30)
 
 
 from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()

X = data["data"]
Y = data["target"]

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X = sc.fit_transform(X)

for i in range(100):
    lr.backpropagate(X, Y)

pred = lr.forward(X)

pred

from sklearn.metrics import accuracy_score

accuracy_score(np.round(pred), Y)

---------------------------------------------------------------------------------
